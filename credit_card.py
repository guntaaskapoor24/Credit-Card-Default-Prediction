# -*- coding: utf-8 -*-
"""Credit_Card.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VDRdOlHPdPEFg83YLJsoVGqWqsvPDRO1

# **This notebook has been created by Guntaas Kapoor**
# **For more details contact me at kapoorguntaas2409@gmail.com**
"""

# Reading in the necessary libraries

"""## **1.1 Importing necessary libraries**"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn
import os
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import xgboost
from xgboost import XGBClassifier
from sklearn.metrics import f1_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import make_scorer
from sklearn.model_selection import StratifiedKFold
!pip install catboost
from catboost import CatBoostClassifier
import lightgbm

# Importing the data

"""## **1.2 Importing the data, data imputation and EDA**"""

data = pd.read_csv('train.csv')
data.head()

data.info()

data.describe().T

# Total rows: 45528

# Checking for missing values and imputing them

data.isnull().sum()

# Owns car - Y, N
# no_of_children - integer
# no_of_days_employed - float
# total_family_members - float
# migrant_worker - float
# yearly_debt_payments - float
# credit_score - float

data['owns_car'].unique()

data['no_of_children'].unique()

sns.histplot(data = data, x = data['no_of_children'], binwidth = 1)
plt.grid()
# 'no_of_children is highly skewed so using median as imputation method'

no_of_children_median = np.nanmedian(data['no_of_children'])
no_of_children_median
data['no_of_children'] = data['no_of_children'].fillna(no_of_children_median)
# nanmedian computes the median of the data while ignoring the nan values

sns.histplot(data = data, x = data['owns_car'], binwidth = 1)
# using mode as the inputation method by observing the most common occuring value, difference between counts is

data['owns_car'].value_counts()
len(data['owns_car'])
y_prop = ((data['owns_car'] == 'Y').sum()/len(data['owns_car']))*100
n_prop = ((data['owns_car'] == 'N').sum()/len(data['owns_car']))*100
print(n_prop, y_prop)
# owns_car == N has almost double the proportion of owns_car == Y

owns_car_mode = data['owns_car'].mode()[0]
data['owns_car'] = data['owns_car'].fillna(owns_car_mode)

sns.histplot(data = data, x = data['total_family_members'], binwidth = 1)
plt.grid()

# need to check the relationship between the no_of_children and total_family_members before any data imputation

sns.scatterplot(data = data, x = data['no_of_children'], y = data['total_family_members'])

total_family_members_median = np.nanmedian(data['total_family_members'])
total_family_members_median
data['total_family_members'] = data['total_family_members'].fillna(total_family_members_median)

sns.kdeplot(data['no_of_children'])

data.isnull().sum()

data['migrant_worker'].value_counts()

# Represents whether a customer is a migrant worker. 1 means yes and 0 means no

data['migrant_worker'].isnull().sum()

sns.displot(data = data, x = data['migrant_worker'], binwidth = 0.5)

# using mode as the imputation method since migrant_worker = 0 is nearly 4 times the number of migrant_worker = 0

migrant_worker_mode = data['migrant_worker'].mode()[0]
migrant_worker_mode
data['migrant_worker'] = data['migrant_worker'].fillna(migrant_worker_mode)

data.isnull().sum()

data['no_of_days_employed'].value_counts()

data['no_of_days_employed'].hist()

# using median as data imputation method
no_of_days_employed_median = np.nanmedian(data['no_of_days_employed'])
data['no_of_days_employed'] = data['no_of_days_employed'].fillna(no_of_days_employed_median)

yearly_debt_payments_median = np.nanmedian(data['yearly_debt_payments'])
yearly_debt_payments_median
data['yearly_debt_payments'] = data['yearly_debt_payments'].fillna(yearly_debt_payments_median)

credit_score_median = np.nanmedian(data['credit_score'])
credit_score_median
data['credit_score'] = data['credit_score'].fillna(credit_score_median)

data.isnull().sum()

# EDA

# Encoding variables

data = data.drop(['name'], axis = 1)

data

"""## **1.3 Encoding the Data**"""

le = LabelEncoder()

data.gender = le.fit_transform(data.gender)
data.owns_car = le.fit_transform(data.owns_car)
data.owns_house = le.fit_transform(data.owns_house)
data.occupation_type = le.fit_transform(data.occupation_type)
data.migrant_worker = le.fit_transform(data.migrant_worker)
data.credit_card_default = le.fit_transform(data.credit_card_default)

data

ax = sns.countplot(data = data, x = data['credit_card_default'])
ax.bar_label(ax.containers[0])
plt.grid()

# The number of people who defaulted on their Credit Card payments is very less and this represents a class imbalance

customer_id = data['customer_id']

customer_id

data = data.drop(['customer_id'], axis = 1)

data

y = data['credit_card_default']
X = data.drop('credit_card_default', axis = 1)

"""## **1.4.1 Train Test Split**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, shuffle = True, stratify = y, random_state = 24)

"""## **1.4.2 Random Forest**"""

from sklearn.ensemble import RandomForestClassifier

model_rf = RandomForestClassifier(random_state = 24, n_estimators = 500)
model_rf.fit(X_train, y_train)

y_predict_rf = model_rf.predict(X_test)

from sklearn.metrics import f1_score
rf_f1_score = f1_score(y_test, y_predict_rf)
rf_f1_score

from sklearn.metrics import recall_score
rf_recall_score = recall_score(y_test, y_predict_rf)
rf_recall_score

conf_matrix_rf = confusion_matrix(y_test, y_predict_rf)
conf_matrix_rf
from sklearn.metrics import ConfusionMatrixDisplay
display_rf = ConfusionMatrixDisplay(conf_matrix, display_labels = ['0', '1'])

display_rf.plot()

"""## **1.4.3 XGBOOST (Base Model)**"""

xgb = XGBClassifier(scale_pos_weight = (y_train == 0).sum()/(y_train == 1).sum(),
                    tree_method='gpu_hist',
                    predictor='gpu_predictor',
                    use_label_encoder = False,
                    eval_metric = 'logloss',
                    learning_rate = 0.1,
                    random_state = 24)

xgb.fit(X_train, y_train)

y_predict_XGB = xgb.predict(X_test)

from sklearn.metrics import f1_score
f1_score_xgBoost_macro = f1_score(y_test, y_predict_XGB, average = 'macro')
f1_score_xgBoost_macro

from sklearn.metrics import recall_score
recall_score_xgboost = recall_score(y_test, y_predict_XGB, average = 'binary', pos_label = 1)
recall_score_xgboost

conf_matrix_xgb = confusion_matrix(y_test, y_predict_XGB)
conf_matrix_xgb
from sklearn.metrics import ConfusionMatrixDisplay
display_xgb = ConfusionMatrixDisplay(conf_matrix_xgb, display_labels = ['0', '1'])

display_xgb.plot()

"""## **1.4.4 XGBOOST (Tuned)**"""

xgb_model = XGBClassifier(scale_pos_weight = (y_train == 0).sum()/(y_train == 1).sum(),
                          tree_method='gpu_hist',
                          predictor='gpu_predictor',
                          use_label_encoder = False, eval_metric = 'logloss', random_state = 24)

param_grid = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.001, 0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'gamma': [0, 0.1],
    'reg_lambda': [1, 10]
}

cv = StratifiedKFold(n_splits = 4, shuffle = True, random_state = 24)
scoring_f1 = {
    'f1': make_scorer(f1_score, average = 'macro')
}

random_search_f1 = RandomizedSearchCV(
    estimator = xgb_model,
    param_distributions = param_grid,
    n_iter = 50,
    scoring = scoring_f1,
    refit = 'f1',
    cv = cv,
    verbose = 1,
    random_state = 24,
    n_jobs = -1
)

random_search_f1.fit(X_train, y_train)

print("Best Parameters:", random_search_f1.best_params_)
print("Best F1 Macro Score:", random_search_f1.best_score_)

cv = StratifiedKFold(n_splits = 4, shuffle = True, random_state = 24)
scoring_recall = {
    'recall': make_scorer(recall_score, average = 'binary', pos_label = 1)
}

random_search_recall = RandomizedSearchCV(
    estimator = xgb_model,
    param_distributions = param_grid,
    n_iter = 50,
    scoring = scoring_recall,
    refit = 'recall',
    cv = cv,
    verbose = 1,
    random_state = 24,
    n_jobs = -1
)

random_search_recall.fit(X_train, y_train)

print("Best parameters: ", random_search_recall.best_params_)
print("Best Recall Score: ", random_search_recall.best_score_)

random_search_recall.cv_results_['mean_test_recall'].max()

"""## **1.4.5 CatBoost (Base Model)**"""

model_catBoost = CatBoostClassifier(class_weights = [1, (y_train == 0).sum()/(y_train == 1).sum()],
                                   verbose = 0,
                                   random_state = 24)

model_catBoost.fit(X_train, y_train)

y_predict_catBoost = model_catBoost.predict(X_test)

from sklearn.metrics import f1_score
f1_score_catBoost_macro = f1_score(y_test, y_predict_catBoost, average = 'macro')
f1_score_catBoost_macro

from sklearn.metrics import recall_score
cat_rs = recall_score(y_test, y_predict_catBoost, average = 'binary', pos_label = 1)
cat_rs

conf_matrix_catBoost = confusion_matrix(y_test, y_predict_catBoost)
conf_matrix_catBoost
from sklearn.metrics import ConfusionMatrixDisplay
display_catBoost = ConfusionMatrixDisplay(conf_matrix_catBoost, display_labels = ['0', '1'])

display_catBoost.plot()

"""## **1.4.6 CatBoost (Tuned)**"""

from catboost import CatBoostClassifier
cv = StratifiedKFold(n_splits = 4, shuffle = True, random_state = 24)
scorer = make_scorer(f1_score, average = 'macro')

class_weights = [(y_train == 0).sum() / len(y_train), (y_train == 1).sum() / len(y_train)]

cat_model = CatBoostClassifier(
    loss_function = 'Logloss',
    class_weights = class_weights,
    verbose = 0,
    random_state = 24
)

cat_param_grid = {
    'depth': [4, 6, 7, 8],
    'learning_rate': [0.01, 0.05, 0.1],
    'iterations': [200, 400],
    'bagging_temperature': [0.0, 0.3, 0.6, 1.0]
}

cat_random_search = RandomizedSearchCV(
    estimator = cat_model,
    param_distributions = cat_param_grid,
    n_iter = 20,
    scoring = scorer,
    cv = cv,
    verbose = 1,
    random_state = 24,
    n_jobs = -1
)

cat_random_search.fit(X_train, y_train)

print("Best Parameters:", cat_random_search.best_params_)
print("Best F1 Macro Score:", cat_random_search.best_score_)

"""## **1.4.7 Light GBM (Base Model)**"""

model_lightgbm = lightgbm.LGBMClassifier(class_weight = 'balanced', random_state = 24)

model_lightgbm.fit(X_train, y_train)

y_predict_lightgbm = model_lightgbm.predict(X_test)

from sklearn.metrics import f1_score
f1_score_lightgbm = f1_score(y_test, y_predict_lightgbm, average = 'macro')
f1_score_lightgbm

from sklearn.metrics import recall_score
recall_lightgbm = recall_score(y_test, y_predict_lightgbm, average = 'macro', pos_label = 1)
recall_lightgbm

conf_matrix_lightgbm = confusion_matrix(y_test, y_predict_lightgbm)
conf_matrix_lightgbm
from sklearn.metrics import ConfusionMatrixDisplay
display_lightgbm = ConfusionMatrixDisplay(conf_matrix_lightgbm, display_labels = ['0', '1'])

display_lightgbm.plot()

"""## **1.4.8 Light GBM (Tuned)**"""

cv = StratifiedKFold(n_splits = 4, shuffle=True, random_state = 24)
scorer = make_scorer(f1_score, average = 'macro')


scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

lightgbm_model = lightgbm.LGBMClassifier(objective = 'binary', scale_pos_weight = scale_pos_weight, random_state = 24)

lightgbm_param_grid = {
    'num_leaves': np.arange(20, 100, 20),
    'learning_rate': np.linspace(0.01, 0.1, 10),
    'n_estimators': np.arange(100, 500, 100),
    'max_depth': np.arange(3, 10),
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

lightgbm_random_search = RandomizedSearchCV(
    estimator = lightgbm_model,
    param_distributions = lightgbm_param_grid,
    n_iter = 50,
    scoring = scorer,
    cv = cv,
    verbose = 1,
    random_state = 42,
    n_jobs = -1
)

lightgbm_random_search.fit(X_train, y_train)

print("Best Parameters:", lightgbm_random_search.best_params_)
print("Best F1 Macro Score:", lightgbm_random_search.best_score_)

"""## **2. Results**"""

# xgb:
# 1. f1 macro from 88.77 to 91.17
# 2. Baseline recall: 98.24 to 99.96

# Random Forest
# 1. f1 macro 86.2624
# 2. Baseline recall: 76.048

# lightgbm
# 1. f1 macro from 90.39 to 91.11
# 2. Baseline recall: 95.60

# catboost
# 1. f1 macro from 90.46 to 92.65
# Baseline Recall: 92.83

"""# **Thank You**
# **This notebook has been created by Guntaas Kapoor**
# **For more details contact me at kapoorguntaas2409@gmail.com**
"""

